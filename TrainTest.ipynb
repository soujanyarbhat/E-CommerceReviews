{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainTest.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdDEDmWlyHE81GeSz3Olrf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0802001352624598bb2dc9d301b24bb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_92591c307ab845c385e157dfce8fe8fe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1dc303366e96486facb8614fc7703ace","IPY_MODEL_39ccf6a513c0478e8ea43e0dbd564c1c"]}},"92591c307ab845c385e157dfce8fe8fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1dc303366e96486facb8614fc7703ace":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_01c54f7c180242f79da7395c0fb8d6e4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f276fef24df24054b550f09d72d2f937"}},"39ccf6a513c0478e8ea43e0dbd564c1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b1810fdccf4470b8bfd66c8267768b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 675kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee1efc09003d4b27a0bd916174e32f0f"}},"01c54f7c180242f79da7395c0fb8d6e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f276fef24df24054b550f09d72d2f937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b1810fdccf4470b8bfd66c8267768b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ee1efc09003d4b27a0bd916174e32f0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6052d28a6ebf44d8952b355a8779d77e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_401569464a104e0fa80bbe8a3e3991ea","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_41615b11e4294c0aa9d7d1641bb8631e","IPY_MODEL_2ec0d32295d347759f4dac710e122fcf"]}},"401569464a104e0fa80bbe8a3e3991ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"41615b11e4294c0aa9d7d1641bb8631e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c702c3beb78a4edead408f0e2f6e796a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4661be22754b42a5ace9f19188ee6d0b"}},"2ec0d32295d347759f4dac710e122fcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_395cac40796c446faa9ff72d9af75e82","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:09&lt;00:00, 44.3B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_343a4223a29d4332a37e2dde03b265bc"}},"c702c3beb78a4edead408f0e2f6e796a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4661be22754b42a5ace9f19188ee6d0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"395cac40796c446faa9ff72d9af75e82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"343a4223a29d4332a37e2dde03b265bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"080015d4b6f749aeab87954b39ee53e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_177fd4273d2d41b5a985d2be0a7e5818","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_920b2a82c6af407bb1ceb9adb8b3fabd","IPY_MODEL_01bb816a7ec14fc1869601088eb94495"]}},"177fd4273d2d41b5a985d2be0a7e5818":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"920b2a82c6af407bb1ceb9adb8b3fabd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_020703a8c5364c09ae702142380ebc68","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10c64b6aa6334f5881c957812b55ba58"}},"01bb816a7ec14fc1869601088eb94495":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77333a1131b941b68f94a6feff5fa125","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:09&lt;00:00, 45.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b4dcacab3bf04a2db022b345390b780d"}},"020703a8c5364c09ae702142380ebc68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"10c64b6aa6334f5881c957812b55ba58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77333a1131b941b68f94a6feff5fa125":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b4dcacab3bf04a2db022b345390b780d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"_6Pj__byqQT4","executionInfo":{"status":"ok","timestamp":1604645720948,"user_tz":420,"elapsed":1502,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtvoH7Ec7UtZ","executionInfo":{"status":"ok","timestamp":1604645747834,"user_tz":420,"elapsed":28376,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"87f682f5-1203-4400-96cc-8f7134463f85","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0WHrfyFK7YL_","executionInfo":{"status":"ok","timestamp":1604645747835,"user_tz":420,"elapsed":28372,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["PARENT_DIR = \"/content/gdrive/My Drive/EBAY\"\n","DATA_DIR = PARENT_DIR + \"/data\"\n","DATA_PATH = DATA_DIR + \"/data.csv\"\n","VER = \"BertWithFeatureClassification\"\n","\n","OUTPUT_DIR = PARENT_DIR + \"/output/\"+VER\n","MODEL_DIR = PARENT_DIR + \"/model/\"+VER\n","\n","MODEL_PATH = MODEL_DIR + \"/pytorch_model.bin\"\n","CONFIG_PATH = MODEL_DIR + \"/config.json\"\n","VOCAB_PATH = MODEL_DIR + \"/vocab.txt\"\n","BERT = \"bert-base-cased\"\n","REPORT_PATH = OUTPUT_DIR + \"/results.txt\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QLTqNJ4-jZ-","executionInfo":{"status":"ok","timestamp":1604645748968,"user_tz":420,"elapsed":29501,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["import os\n","if not os.path.exists(MODEL_DIR):\n","  os.makedirs(MODEL_DIR)\n","if not os.path.exists(OUTPUT_DIR):\n","  os.makedirs(OUTPUT_DIR)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywvaemdS-mwo","executionInfo":{"status":"ok","timestamp":1604645748971,"user_tz":420,"elapsed":29499,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["batch_size = 8\n","max_len = 150\n","epochs = 20\n","max_grad_norm = 1.0\n","full_finetuning = False\n","lr = 3e-5"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpqG2p_X_Il4","executionInfo":{"status":"ok","timestamp":1604645761784,"user_tz":420,"elapsed":42307,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"135ac2d2-844a-4263-c5c0-76cad43cce47","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install seqeval\n","!pip install transformers"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=3ff900cdc3ba661ca5f05c8ee56db0175ec9fe64c89f855687a452cefb941c2a\n","  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers==0.9.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 43.1MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 43.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 38.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=e160a4c509e8062ab29ccebd3d413616c9db0f3f1d669e077aa6f54e2e09cf9f\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gyQc6sxt_Jot","executionInfo":{"status":"ok","timestamp":1604645768838,"user_tz":420,"elapsed":49357,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["import pandas as pd\n","import math\n","import numpy as np\n","from seqeval.metrics import f1_score\n","from sklearn.metrics import classification_report,accuracy_score,f1_score, multilabel_confusion_matrix\n","import torch.nn.functional as F\n","\n","import torch\n","import os\n","from tqdm import tqdm,trange\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, TensorDataset\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertModel, BertTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","import torch.nn as nn\n","import seaborn as sns"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xx8no-J5A_Kg","executionInfo":{"status":"ok","timestamp":1604645770108,"user_tz":420,"elapsed":50623,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"a6b5ec05-e378-4299-88d8-eb9988d6cffc","colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["df = pd.read_csv(DATA_PATH)\n","\n","# creating instance of labelencoder\n","labelencoder = LabelEncoder()\n","\n","# Assigning numerical values for decades\n","df['label'] = labelencoder.fit_transform(df['Decade'])\n","num_of_labels = len(df['label'].unique())\n","classes = list(labelencoder.classes_)\n","df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Decade</th>\n","      <th>Age</th>\n","      <th>Title</th>\n","      <th>Review Text</th>\n","      <th>Division Name</th>\n","      <th>Department Name</th>\n","      <th>Class Name</th>\n","      <th>temp</th>\n","      <th>Text</th>\n","      <th>Word Count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1980</td>\n","      <td>33</td>\n","      <td>NaN</td>\n","      <td>Absolutely wonderful - silky and sexy and comf...</td>\n","      <td>Intimates</td>\n","      <td>Intimate</td>\n","      <td>Intimates</td>\n","      <td>NaN</td>\n","      <td>Title: , Division: Intimates, Department: Inti...</td>\n","      <td>8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1980</td>\n","      <td>34</td>\n","      <td>NaN</td>\n","      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n","      <td>General</td>\n","      <td>Dresses</td>\n","      <td>Dresses</td>\n","      <td>NaN</td>\n","      <td>Title: , Division: General, Department: Dresse...</td>\n","      <td>62</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1960</td>\n","      <td>60</td>\n","      <td>Some major design flaws</td>\n","      <td>I had such high hopes for this dress and reall...</td>\n","      <td>General</td>\n","      <td>Dresses</td>\n","      <td>Dresses</td>\n","      <td>Some major design flaws</td>\n","      <td>Title: Some major design flaws, Division: Gene...</td>\n","      <td>98</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1970</td>\n","      <td>50</td>\n","      <td>My favorite buy!</td>\n","      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n","      <td>General Petite</td>\n","      <td>Bottoms</td>\n","      <td>Pants</td>\n","      <td>My favorite buy!</td>\n","      <td>Title: My favorite buy!, Division: General Pet...</td>\n","      <td>22</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1970</td>\n","      <td>47</td>\n","      <td>Flattering shirt</td>\n","      <td>This shirt is very flattering to all due to th...</td>\n","      <td>General</td>\n","      <td>Tops</td>\n","      <td>Blouses</td>\n","      <td>Flattering shirt</td>\n","      <td>Title: Flattering shirt, Division: General, De...</td>\n","      <td>36</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Decade  Age  ... Word Count label\n","0    1980   33  ...          8     6\n","1    1980   34  ...         62     6\n","2    1960   60  ...         98     4\n","3    1970   50  ...         22     5\n","4    1970   47  ...         36     5\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"LdehxokgERsK","executionInfo":{"status":"ok","timestamp":1604645770109,"user_tz":420,"elapsed":50621,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"823ad6d4-4e24-4aa5-e684-49b54607689d","colab":{"base_uri":"https://localhost:8080/"}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","n_gpu"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"rro_Zi0vE_5C","executionInfo":{"status":"ok","timestamp":1604645770111,"user_tz":420,"elapsed":50619,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["# Splitting into train val and test splits\n","train_text, temp_text, train_labels, temp_labels = train_test_split(df['Text'], df['label'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.3, \n","                                                                    stratify=df['label'])\n","\n","\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXqbvWX3l-Xv","executionInfo":{"status":"ok","timestamp":1604645770112,"user_tz":420,"elapsed":50617,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"07e3788d-a20d-4665-b0e6-c2858505e738","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(train_text)\n","print(train_labels)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["21285    Title: Classic, Division: General, Department:...\n","19034    Title: A tad thin and see through, Division: G...\n","22154    Title: Failure, Division: General Petite, Depa...\n","5378     Title: Frankenstein experiment gone wrong, Div...\n","6909     Title: Beautiful design, tight fit, Division: ...\n","                               ...                        \n","21304    Title: Beautiful dress, Division: General Peti...\n","16418    Title: , Division: General, Department: Tops, ...\n","7368     Title: Soft blouse, Division: General, Departm...\n","6050     Title: You will love this!, Division: General ...\n","10761    Title: So cozy and lovely!, Division: General,...\n","Name: Text, Length: 15848, dtype: object\n","21285    7\n","19034    6\n","22154    3\n","5378     5\n","6909     5\n","        ..\n","21304    4\n","16418    5\n","7368     7\n","6050     4\n","10761    5\n","Name: label, Length: 15848, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ApV1mWAjIskn","executionInfo":{"status":"ok","timestamp":1604645771363,"user_tz":420,"elapsed":51865,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"77925b90-ac18-47d4-80a8-5c8b2889df59","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["0802001352624598bb2dc9d301b24bb3","92591c307ab845c385e157dfce8fe8fe","1dc303366e96486facb8614fc7703ace","39ccf6a513c0478e8ea43e0dbd564c1c","01c54f7c180242f79da7395c0fb8d6e4","f276fef24df24054b550f09d72d2f937","1b1810fdccf4470b8bfd66c8267768b6","ee1efc09003d4b27a0bd916174e32f0f"]}},"source":["# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(BERT,do_lower_case=False)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0802001352624598bb2dc9d301b24bb3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TGdH1gwWI9ei","executionInfo":{"status":"ok","timestamp":1604645800861,"user_tz":420,"elapsed":81358,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"28c47ce1-f201-4aa2-a6f3-a7fbb2622137","colab":{"base_uri":"https://localhost:8080/"}},"source":["tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_len,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_len,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_len,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","## convert lists to tensors\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YPYeDZAbg1-G","executionInfo":{"status":"ok","timestamp":1604645800862,"user_tz":420,"elapsed":81355,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"9ba19f58-12bb-48cf-ae7e-38ae3bb0c9cb","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(f\"Train size: {train_seq.shape}, Val size: {val_seq.shape}, Test size: {test_seq.shape}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train size: torch.Size([15848, 150]), Val size: torch.Size([3396, 150]), Test size: torch.Size([3397, 150])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pDKAE2oBdNWu","executionInfo":{"status":"ok","timestamp":1604645813168,"user_tz":420,"elapsed":93658,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"d480bd30-af30-47df-bd20-b84cc09c63c1","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(\"Class Weights:\",class_weights)\n","\n","# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.CrossEntropyLoss() "],"execution_count":15,"outputs":[{"output_type":"stream","text":["Class Weights: [195.65432099  29.34814815   9.07674685   1.25241031   0.66473722\n","   0.44099396   0.3333123    0.81901809  16.93162393]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EbUwhJtsL1ox","executionInfo":{"status":"ok","timestamp":1604645826843,"user_tz":420,"elapsed":107330,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"75e83d3c-782b-4697-9228-08b3147e3657","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["6052d28a6ebf44d8952b355a8779d77e","401569464a104e0fa80bbe8a3e3991ea","41615b11e4294c0aa9d7d1641bb8631e","2ec0d32295d347759f4dac710e122fcf","c702c3beb78a4edead408f0e2f6e796a","4661be22754b42a5ace9f19188ee6d0b","395cac40796c446faa9ff72d9af75e82","343a4223a29d4332a37e2dde03b265bc","080015d4b6f749aeab87954b39ee53e7","177fd4273d2d41b5a985d2be0a7e5818","920b2a82c6af407bb1ceb9adb8b3fabd","01bb816a7ec14fc1869601088eb94495","020703a8c5364c09ae702142380ebc68","10c64b6aa6334f5881c957812b55ba58","77333a1131b941b68f94a6feff5fa125","b4dcacab3bf04a2db022b345390b780d"]}},"source":["# model = BertForSequenceClassification.from_pretrained(BERT, num_labels=num_of_labels)\n","bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","for param in bert.parameters():\n","    param.requires_grad = False\n","\n","class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert, num_of_labels):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      self.fc1 = nn.Linear(768,512)\n","      \n","      # dense layer decades (Output layer)\n","      self.fc2 = nn.Linear(512,num_of_labels)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","      \n","      x = self.fc1(cls_hs)\n","\n","      x = self.relu(x)\n","\n","      x = self.dropout(x)\n","\n","      # output layer\n","      x = self.fc2(x)\n","      \n","      return x\n","\n","model = BERT_Arch(bert, len(classes))"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6052d28a6ebf44d8952b355a8779d77e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"080015d4b6f749aeab87954b39ee53e7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lnPu1RX3NOLz","executionInfo":{"status":"ok","timestamp":1604645826845,"user_tz":420,"elapsed":107322,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["model.cuda();"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCUUvp7nMj3F","executionInfo":{"status":"ok","timestamp":1604645826846,"user_tz":420,"elapsed":107314,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["optimizer = AdamW(model.parameters(), lr=lr)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"geRL9xyzdXfe","executionInfo":{"status":"ok","timestamp":1604645826847,"user_tz":420,"elapsed":107307,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["# function to train the model\n","def train():\n","  print(\"\\nTraining...\")\n","  \n","  model.train()\n","\n","  total_loss = 0\n","\n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    b_input_ids, b_input_mask, b_labels = batch    \n","\n","    # clear previously calculated gradients \n","    model.zero_grad()      \n","\n","    # get model predictions for the current batch\n","    outputs = model(b_input_ids, b_input_mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(outputs.view(-1, num_of_labels), b_labels.view(-1))\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  return avg_loss"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wx8oYY9Tdp7R","executionInfo":{"status":"ok","timestamp":1604645826847,"user_tz":420,"elapsed":107298,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss = 0\n","  \n","  # empty list to save the model predictions\n","  y_pred, y_true = [], []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      outputs = model(b_input_ids, b_input_mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(outputs.view(-1, num_of_labels), b_labels.view(-1))\n","\n","      total_loss = total_loss + loss.item()\n","\n","      y_pred.extend(torch.argmax(outputs, 1).tolist())\n","      y_true.extend(b_labels.tolist())\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader)\n","\n","  print('Classification Report:')\n","  print(classification_report(y_true, y_pred, target_names=[str(c) for c in classes], digits=4))\n","\n","  return avg_loss, y_true, y_pred"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCwTWiwTdurr"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for _ in trange(epochs,desc=\"Epoch\"):\n","    \n","    #train model\n","    train_loss = train()\n","\n","    # append training loss\n","    train_losses.append(train_loss)\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","\n","    #evaluate model\n","    valid_loss, _, _ = evaluate()\n","\n","    valid_losses.append(valid_loss)\n","\n","    print(f'Validation Loss: {valid_loss:.3f}')\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), MODEL_DIR+'/saved_weights.pt')\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QQv3tGCBavgV","executionInfo":{"status":"ok","timestamp":1604645851037,"user_tz":420,"elapsed":2110,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"3eb72cbb-5396-4b1a-82f1-781c691efe69","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls '$MODEL_DIR'"],"execution_count":22,"outputs":[{"output_type":"stream","text":["saved_weights.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kp70dbjQax3o","executionInfo":{"status":"aborted","timestamp":1604645844515,"user_tz":420,"elapsed":124947,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(train_losses, 'b-o', label=\"training loss\")\n","plt.plot(valid_losses, 'r-o', label=\"validation loss\")\n","\n","# Label the plot.\n","plt.title(\"Learning curve\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","\n","plt.savefig(OUTPUT_DIR + \"/loss.png\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hu9_vN44D2yW"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"EujeNY4RD1xl","executionInfo":{"status":"ok","timestamp":1604645932127,"user_tz":420,"elapsed":75136,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"cd636b2b-05ab-4ca0-d335-ebde6109a59f","colab":{"base_uri":"https://localhost:8080/"}},"source":["path = MODEL_DIR + '/saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n","\n","# wrap tensors\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","\n","# sampler for sampling the data during training\n","test_sampler = SequentialSampler(test_data)\n","\n","# dataLoader for validation set\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n","\n","print(\"\\nTesting...\")\n","  \n","# deactivate dropout layers\n","model.eval()\n","\n","# empty list to save the model predictions\n","y_pred, y_true = [], []\n","\n","# iterate over batches\n","for step,batch in enumerate(test_dataloader):\n","\n","  # push the batch to gpu\n","  batch = [t.to(device) for t in batch]\n","\n","  b_input_ids, b_input_mask, b_labels = batch\n","\n","  # deactivate autograd\n","  with torch.no_grad():\n","    \n","    # model predictions\n","    outputs = model(b_input_ids, b_input_mask)\n","\n","    y_pred.extend(torch.argmax(outputs, 1).tolist())\n","    y_true.extend(b_labels.tolist())\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","Testing...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hrilWnXLW7oq","executionInfo":{"status":"ok","timestamp":1604645934456,"user_tz":420,"elapsed":2321,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"603ee41b-9177-4151-cb77-59c4cc831edf","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Get acc , recall, F1 result report\n","report = classification_report(y_true, y_pred, zero_division=1, digits=4)\n","\n","\n","# Save the report into file\n","with open(REPORT_PATH, \"w\") as writer:\n","    print(\"***** Eval results(Lenient) *****\")\n","    print(\"\\n%s\"%(report))\n","    print(\"F1 score: %f\"%(f1_score(y_true, y_pred, average='micro')))\n","    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n","    \n","    writer.write(\"F1 score(Lenient):\\n\")\n","    writer.write(str(f1_score(y_true, y_pred, average='micro')))\n","    writer.write(\"\\n\\nAccuracy score:\\n\")\n","    writer.write(str(accuracy_score(y_true, y_pred)))\n","    writer.write(\"\\n\\n\")  \n","    writer.write(report)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["***** Eval results(Lenient) *****\n","\n","              precision    recall  f1-score   support\n","\n","           0     1.0000    0.0000    0.0000         1\n","           1     1.0000    0.0000    0.0000        13\n","           2     1.0000    0.0000    0.0000        41\n","           3     1.0000    0.0000    0.0000       302\n","           4     1.0000    0.0000    0.0000       568\n","           5     1.0000    0.0000    0.0000       856\n","           6     0.3335    1.0000    0.5002      1133\n","           7     1.0000    0.0000    0.0000       461\n","           8     1.0000    0.0000    0.0000        22\n","\n","    accuracy                         0.3335      3397\n","   macro avg     0.9259    0.1111    0.0556      3397\n","weighted avg     0.7777    0.3335    0.1668      3397\n","\n","F1 score: 0.333530\n","Accuracy score: 0.333530\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iUfUweqkoXJW","executionInfo":{"status":"ok","timestamp":1604645934457,"user_tz":420,"elapsed":2316,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"575e5d5b-b856-4281-df94-a6cf985ccdaf","colab":{"base_uri":"https://localhost:8080/"}},"source":["print([(x,y) for x,y in zip(y_true, y_pred)])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[(4, 6), (4, 6), (4, 6), (6, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (2, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (3, 6), (4, 6), (6, 6), (6, 6), (5, 6), (4, 6), (3, 6), (5, 6), (3, 6), (4, 6), (3, 6), (5, 6), (7, 6), (6, 6), (4, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (7, 6), (5, 6), (3, 6), (3, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (7, 6), (4, 6), (7, 6), (5, 6), (6, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (4, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 6), (4, 6), (4, 6), (6, 6), (4, 6), (5, 6), (4, 6), (3, 6), (6, 6), (6, 6), (7, 6), (2, 6), (4, 6), (4, 6), (7, 6), (6, 6), (4, 6), (3, 6), (2, 6), (5, 6), (6, 6), (3, 6), (7, 6), (7, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (3, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (5, 6), (5, 6), (3, 6), (6, 6), (6, 6), (2, 6), (7, 6), (6, 6), (7, 6), (5, 6), (7, 6), (5, 6), (6, 6), (5, 6), (4, 6), (8, 6), (7, 6), (6, 6), (3, 6), (7, 6), (7, 6), (6, 6), (5, 6), (4, 6), (3, 6), (7, 6), (7, 6), (7, 6), (5, 6), (4, 6), (4, 6), (4, 6), (6, 6), (6, 6), (6, 6), (7, 6), (6, 6), (5, 6), (3, 6), (6, 6), (6, 6), (6, 6), (4, 6), (7, 6), (6, 6), (7, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (6, 6), (7, 6), (7, 6), (5, 6), (5, 6), (5, 6), (5, 6), (7, 6), (7, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (4, 6), (5, 6), (4, 6), (5, 6), (6, 6), (3, 6), (5, 6), (5, 6), (6, 6), (4, 6), (2, 6), (5, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (4, 6), (6, 6), (4, 6), (5, 6), (6, 6), (3, 6), (5, 6), (4, 6), (3, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (7, 6), (6, 6), (4, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (5, 6), (6, 6), (5, 6), (6, 6), (7, 6), (3, 6), (5, 6), (5, 6), (3, 6), (6, 6), (5, 6), (4, 6), (4, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (7, 6), (7, 6), (4, 6), (4, 6), (6, 6), (5, 6), (7, 6), (6, 6), (1, 6), (4, 6), (5, 6), (3, 6), (5, 6), (7, 6), (0, 6), (4, 6), (6, 6), (4, 6), (6, 6), (7, 6), (5, 6), (4, 6), (6, 6), (4, 6), (3, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (3, 6), (7, 6), (6, 6), (6, 6), (4, 6), (7, 6), (7, 6), (4, 6), (4, 6), (6, 6), (6, 6), (5, 6), (3, 6), (7, 6), (5, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (2, 6), (5, 6), (5, 6), (7, 6), (5, 6), (6, 6), (3, 6), (3, 6), (7, 6), (6, 6), (4, 6), (4, 6), (5, 6), (3, 6), (6, 6), (4, 6), (6, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (3, 6), (6, 6), (6, 6), (4, 6), (4, 6), (5, 6), (3, 6), (6, 6), (4, 6), (7, 6), (5, 6), (5, 6), (7, 6), (6, 6), (5, 6), (4, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (4, 6), (5, 6), (7, 6), (6, 6), (7, 6), (5, 6), (5, 6), (5, 6), (5, 6), (5, 6), (3, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (3, 6), (3, 6), (7, 6), (3, 6), (4, 6), (7, 6), (5, 6), (4, 6), (6, 6), (7, 6), (6, 6), (6, 6), (3, 6), (2, 6), (6, 6), (7, 6), (4, 6), (7, 6), (3, 6), (6, 6), (7, 6), (6, 6), (4, 6), (7, 6), (7, 6), (8, 6), (3, 6), (7, 6), (6, 6), (7, 6), (6, 6), (5, 6), (3, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (3, 6), (7, 6), (6, 6), (7, 6), (3, 6), (4, 6), (7, 6), (5, 6), (4, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (4, 6), (6, 6), (6, 6), (6, 6), (7, 6), (6, 6), (6, 6), (6, 6), (4, 6), (4, 6), (4, 6), (7, 6), (5, 6), (6, 6), (5, 6), (4, 6), (8, 6), (7, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (4, 6), (5, 6), (5, 6), (3, 6), (6, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (5, 6), (5, 6), (3, 6), (7, 6), (5, 6), (7, 6), (6, 6), (7, 6), (3, 6), (6, 6), (7, 6), (5, 6), (1, 6), (5, 6), (5, 6), (7, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (3, 6), (5, 6), (3, 6), (6, 6), (4, 6), (6, 6), (6, 6), (3, 6), (6, 6), (3, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (4, 6), (4, 6), (6, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (4, 6), (4, 6), (6, 6), (5, 6), (2, 6), (5, 6), (5, 6), (5, 6), (6, 6), (4, 6), (5, 6), (6, 6), (4, 6), (7, 6), (4, 6), (4, 6), (3, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (3, 6), (6, 6), (6, 6), (5, 6), (3, 6), (4, 6), (7, 6), (5, 6), (5, 6), (7, 6), (7, 6), (5, 6), (4, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (7, 6), (6, 6), (5, 6), (2, 6), (5, 6), (5, 6), (6, 6), (7, 6), (5, 6), (7, 6), (8, 6), (5, 6), (6, 6), (6, 6), (8, 6), (4, 6), (6, 6), (5, 6), (7, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (7, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (3, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (5, 6), (5, 6), (5, 6), (3, 6), (3, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (4, 6), (3, 6), (5, 6), (4, 6), (6, 6), (6, 6), (7, 6), (4, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (4, 6), (7, 6), (6, 6), (6, 6), (6, 6), (5, 6), (7, 6), (4, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (7, 6), (6, 6), (5, 6), (4, 6), (3, 6), (6, 6), (4, 6), (3, 6), (3, 6), (4, 6), (6, 6), (5, 6), (6, 6), (4, 6), (5, 6), (6, 6), (7, 6), (5, 6), (5, 6), (5, 6), (6, 6), (5, 6), (5, 6), (4, 6), (4, 6), (6, 6), (6, 6), (3, 6), (5, 6), (6, 6), (7, 6), (5, 6), (5, 6), (4, 6), (4, 6), (5, 6), (6, 6), (3, 6), (7, 6), (4, 6), (6, 6), (8, 6), (6, 6), (4, 6), (4, 6), (5, 6), (3, 6), (5, 6), (5, 6), (4, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (4, 6), (7, 6), (6, 6), (6, 6), (5, 6), (7, 6), (6, 6), (7, 6), (5, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (3, 6), (3, 6), (4, 6), (6, 6), (3, 6), (8, 6), (4, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (7, 6), (4, 6), (3, 6), (6, 6), (7, 6), (3, 6), (6, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (7, 6), (4, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (5, 6), (5, 6), (4, 6), (6, 6), (7, 6), (4, 6), (6, 6), (5, 6), (5, 6), (7, 6), (5, 6), (3, 6), (4, 6), (6, 6), (7, 6), (3, 6), (7, 6), (3, 6), (5, 6), (7, 6), (4, 6), (6, 6), (6, 6), (5, 6), (5, 6), (4, 6), (4, 6), (5, 6), (5, 6), (5, 6), (5, 6), (3, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (7, 6), (6, 6), (4, 6), (3, 6), (6, 6), (4, 6), (6, 6), (2, 6), (5, 6), (6, 6), (7, 6), (6, 6), (6, 6), (7, 6), (5, 6), (5, 6), (5, 6), (8, 6), (6, 6), (3, 6), (7, 6), (6, 6), (5, 6), (6, 6), (4, 6), (5, 6), (3, 6), (6, 6), (6, 6), (6, 6), (5, 6), (7, 6), (6, 6), (7, 6), (4, 6), (5, 6), (5, 6), (6, 6), (7, 6), (3, 6), (4, 6), (5, 6), (6, 6), (5, 6), (4, 6), (4, 6), (7, 6), (7, 6), (6, 6), (7, 6), (5, 6), (5, 6), (3, 6), (4, 6), (3, 6), (6, 6), (7, 6), (2, 6), (7, 6), (7, 6), (7, 6), (6, 6), (5, 6), (5, 6), (7, 6), (5, 6), (6, 6), (6, 6), (5, 6), (3, 6), (4, 6), (5, 6), (3, 6), (5, 6), (6, 6), (5, 6), (7, 6), (3, 6), (3, 6), (7, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (4, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (6, 6), (5, 6), (4, 6), (5, 6), (4, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (5, 6), (5, 6), (3, 6), (6, 6), (5, 6), (4, 6), (5, 6), (3, 6), (7, 6), (7, 6), (5, 6), (6, 6), (5, 6), (3, 6), (6, 6), (5, 6), (5, 6), (5, 6), (5, 6), (4, 6), (7, 6), (6, 6), (7, 6), (6, 6), (4, 6), (4, 6), (6, 6), (7, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (4, 6), (4, 6), (3, 6), (7, 6), (3, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (7, 6), (3, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (3, 6), (5, 6), (3, 6), (5, 6), (3, 6), (4, 6), (5, 6), (4, 6), (3, 6), (6, 6), (4, 6), (3, 6), (6, 6), (6, 6), (5, 6), (5, 6), (3, 6), (4, 6), (7, 6), (5, 6), (6, 6), (3, 6), (6, 6), (7, 6), (6, 6), (5, 6), (7, 6), (5, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (7, 6), (4, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (4, 6), (7, 6), (3, 6), (5, 6), (6, 6), (6, 6), (5, 6), (4, 6), (6, 6), (4, 6), (6, 6), (6, 6), (7, 6), (7, 6), (6, 6), (5, 6), (2, 6), (4, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (7, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (7, 6), (4, 6), (5, 6), (4, 6), (7, 6), (7, 6), (4, 6), (5, 6), (7, 6), (6, 6), (6, 6), (3, 6), (4, 6), (3, 6), (6, 6), (4, 6), (7, 6), (5, 6), (5, 6), (5, 6), (7, 6), (5, 6), (2, 6), (6, 6), (5, 6), (3, 6), (6, 6), (7, 6), (2, 6), (4, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (2, 6), (7, 6), (3, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (6, 6), (5, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (4, 6), (7, 6), (7, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (4, 6), (5, 6), (4, 6), (4, 6), (5, 6), (5, 6), (7, 6), (5, 6), (4, 6), (6, 6), (5, 6), (5, 6), (3, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (3, 6), (6, 6), (8, 6), (6, 6), (4, 6), (6, 6), (5, 6), (7, 6), (6, 6), (6, 6), (6, 6), (3, 6), (4, 6), (3, 6), (5, 6), (3, 6), (5, 6), (4, 6), (6, 6), (5, 6), (3, 6), (5, 6), (6, 6), (5, 6), (7, 6), (7, 6), (6, 6), (6, 6), (7, 6), (6, 6), (7, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (3, 6), (7, 6), (6, 6), (4, 6), (7, 6), (3, 6), (5, 6), (5, 6), (5, 6), (4, 6), (7, 6), (6, 6), (6, 6), (7, 6), (6, 6), (3, 6), (6, 6), (4, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (3, 6), (7, 6), (6, 6), (5, 6), (3, 6), (3, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (3, 6), (3, 6), (7, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (3, 6), (3, 6), (4, 6), (6, 6), (3, 6), (4, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (6, 6), (7, 6), (7, 6), (6, 6), (7, 6), (4, 6), (5, 6), (5, 6), (3, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (4, 6), (3, 6), (3, 6), (7, 6), (6, 6), (6, 6), (5, 6), (8, 6), (4, 6), (6, 6), (6, 6), (7, 6), (4, 6), (3, 6), (5, 6), (5, 6), (6, 6), (5, 6), (2, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (7, 6), (6, 6), (7, 6), (6, 6), (7, 6), (4, 6), (6, 6), (6, 6), (5, 6), (7, 6), (5, 6), (7, 6), (6, 6), (5, 6), (4, 6), (3, 6), (4, 6), (6, 6), (7, 6), (5, 6), (4, 6), (5, 6), (6, 6), (6, 6), (5, 6), (2, 6), (6, 6), (5, 6), (4, 6), (6, 6), (6, 6), (6, 6), (4, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (6, 6), (4, 6), (5, 6), (4, 6), (5, 6), (6, 6), (4, 6), (6, 6), (4, 6), (6, 6), (3, 6), (5, 6), (6, 6), (3, 6), (7, 6), (6, 6), (4, 6), (6, 6), (4, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (5, 6), (5, 6), (5, 6), (6, 6), (5, 6), (6, 6), (3, 6), (5, 6), (6, 6), (5, 6), (7, 6), (4, 6), (1, 6), (3, 6), (6, 6), (4, 6), (6, 6), (3, 6), (6, 6), (5, 6), (4, 6), (2, 6), (3, 6), (5, 6), (6, 6), (3, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (5, 6), (5, 6), (7, 6), (4, 6), (4, 6), (4, 6), (6, 6), (1, 6), (3, 6), (2, 6), (4, 6), (4, 6), (6, 6), (6, 6), (7, 6), (1, 6), (6, 6), (7, 6), (4, 6), (3, 6), (8, 6), (4, 6), (5, 6), (6, 6), (6, 6), (4, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (4, 6), (5, 6), (3, 6), (6, 6), (6, 6), (4, 6), (7, 6), (6, 6), (3, 6), (7, 6), (6, 6), (5, 6), (5, 6), (4, 6), (6, 6), (5, 6), (7, 6), (6, 6), (7, 6), (3, 6), (6, 6), (3, 6), (4, 6), (4, 6), (6, 6), (6, 6), (3, 6), (6, 6), (5, 6), (8, 6), (6, 6), (4, 6), (6, 6), (6, 6), (5, 6), (3, 6), (7, 6), (4, 6), (7, 6), (5, 6), (6, 6), (4, 6), (6, 6), (6, 6), (3, 6), (4, 6), (4, 6), (3, 6), (5, 6), (6, 6), (7, 6), (5, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (5, 6), (6, 6), (3, 6), (6, 6), (7, 6), (5, 6), (5, 6), (7, 6), (5, 6), (3, 6), (5, 6), (5, 6), (3, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (7, 6), (6, 6), (4, 6), (5, 6), (3, 6), (4, 6), (2, 6), (5, 6), (4, 6), (4, 6), (6, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (4, 6), (6, 6), (4, 6), (7, 6), (7, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (4, 6), (7, 6), (3, 6), (7, 6), (6, 6), (6, 6), (4, 6), (6, 6), (7, 6), (3, 6), (6, 6), (4, 6), (3, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (3, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (5, 6), (2, 6), (6, 6), (4, 6), (5, 6), (7, 6), (4, 6), (7, 6), (4, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (5, 6), (3, 6), (4, 6), (5, 6), (5, 6), (4, 6), (4, 6), (6, 6), (7, 6), (3, 6), (6, 6), (4, 6), (4, 6), (7, 6), (7, 6), (3, 6), (4, 6), (6, 6), (6, 6), (6, 6), (3, 6), (7, 6), (4, 6), (6, 6), (3, 6), (5, 6), (6, 6), (4, 6), (3, 6), (5, 6), (3, 6), (5, 6), (7, 6), (6, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (6, 6), (5, 6), (5, 6), (4, 6), (6, 6), (4, 6), (6, 6), (6, 6), (7, 6), (6, 6), (4, 6), (6, 6), (5, 6), (5, 6), (4, 6), (6, 6), (5, 6), (6, 6), (7, 6), (4, 6), (6, 6), (5, 6), (6, 6), (3, 6), (6, 6), (3, 6), (7, 6), (7, 6), (7, 6), (6, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (7, 6), (4, 6), (4, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (5, 6), (3, 6), (6, 6), (5, 6), (4, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (7, 6), (7, 6), (6, 6), (5, 6), (5, 6), (6, 6), (5, 6), (4, 6), (7, 6), (6, 6), (7, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (4, 6), (6, 6), (4, 6), (6, 6), (4, 6), (6, 6), (6, 6), (5, 6), (5, 6), (4, 6), (3, 6), (4, 6), (5, 6), (6, 6), (6, 6), (6, 6), (2, 6), (4, 6), (2, 6), (6, 6), (7, 6), (4, 6), (5, 6), (6, 6), (3, 6), (7, 6), (6, 6), (5, 6), (7, 6), (5, 6), (3, 6), (7, 6), (5, 6), (7, 6), (5, 6), (7, 6), (3, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (3, 6), (5, 6), (5, 6), (6, 6), (5, 6), (4, 6), (3, 6), (4, 6), (6, 6), (6, 6), (5, 6), (3, 6), (6, 6), (5, 6), (5, 6), (5, 6), (5, 6), (5, 6), (6, 6), (7, 6), (5, 6), (6, 6), (7, 6), (6, 6), (7, 6), (8, 6), (5, 6), (4, 6), (6, 6), (7, 6), (7, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (3, 6), (5, 6), (6, 6), (6, 6), (7, 6), (6, 6), (6, 6), (5, 6), (6, 6), (4, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (4, 6), (3, 6), (3, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (7, 6), (5, 6), (2, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (2, 6), (4, 6), (7, 6), (6, 6), (5, 6), (7, 6), (6, 6), (4, 6), (4, 6), (3, 6), (5, 6), (7, 6), (7, 6), (6, 6), (7, 6), (3, 6), (7, 6), (4, 6), (1, 6), (4, 6), (5, 6), (3, 6), (5, 6), (3, 6), (4, 6), (3, 6), (3, 6), (4, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (3, 6), (5, 6), (7, 6), (4, 6), (4, 6), (6, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (4, 6), (7, 6), (5, 6), (6, 6), (5, 6), (7, 6), (5, 6), (4, 6), (6, 6), (4, 6), (3, 6), (6, 6), (7, 6), (5, 6), (5, 6), (3, 6), (7, 6), (6, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (3, 6), (6, 6), (7, 6), (5, 6), (6, 6), (4, 6), (8, 6), (6, 6), (5, 6), (4, 6), (3, 6), (4, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (7, 6), (5, 6), (5, 6), (5, 6), (7, 6), (3, 6), (5, 6), (5, 6), (6, 6), (7, 6), (7, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (7, 6), (5, 6), (7, 6), (7, 6), (5, 6), (7, 6), (6, 6), (6, 6), (5, 6), (7, 6), (7, 6), (4, 6), (5, 6), (4, 6), (6, 6), (7, 6), (5, 6), (4, 6), (6, 6), (5, 6), (6, 6), (7, 6), (7, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (7, 6), (7, 6), (4, 6), (3, 6), (7, 6), (6, 6), (7, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (6, 6), (5, 6), (6, 6), (6, 6), (4, 6), (3, 6), (3, 6), (6, 6), (7, 6), (5, 6), (7, 6), (7, 6), (6, 6), (6, 6), (7, 6), (7, 6), (5, 6), (3, 6), (5, 6), (6, 6), (3, 6), (5, 6), (5, 6), (7, 6), (4, 6), (6, 6), (5, 6), (5, 6), (7, 6), (4, 6), (5, 6), (3, 6), (6, 6), (5, 6), (2, 6), (7, 6), (6, 6), (7, 6), (4, 6), (6, 6), (6, 6), (4, 6), (4, 6), (2, 6), (4, 6), (6, 6), (5, 6), (7, 6), (6, 6), (5, 6), (3, 6), (4, 6), (6, 6), (5, 6), (7, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (5, 6), (6, 6), (7, 6), (4, 6), (7, 6), (6, 6), (6, 6), (7, 6), (6, 6), (6, 6), (7, 6), (5, 6), (7, 6), (4, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (3, 6), (4, 6), (6, 6), (7, 6), (5, 6), (4, 6), (4, 6), (5, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (2, 6), (5, 6), (4, 6), (4, 6), (7, 6), (6, 6), (5, 6), (5, 6), (4, 6), (6, 6), (3, 6), (7, 6), (7, 6), (2, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (4, 6), (7, 6), (4, 6), (5, 6), (7, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (4, 6), (6, 6), (5, 6), (5, 6), (7, 6), (5, 6), (7, 6), (6, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (5, 6), (7, 6), (3, 6), (5, 6), (4, 6), (1, 6), (3, 6), (4, 6), (5, 6), (5, 6), (4, 6), (5, 6), (3, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (7, 6), (5, 6), (4, 6), (5, 6), (7, 6), (6, 6), (2, 6), (7, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (7, 6), (5, 6), (4, 6), (5, 6), (4, 6), (5, 6), (3, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (3, 6), (3, 6), (3, 6), (5, 6), (7, 6), (5, 6), (7, 6), (6, 6), (4, 6), (2, 6), (6, 6), (5, 6), (6, 6), (6, 6), (3, 6), (5, 6), (5, 6), (6, 6), (7, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (5, 6), (3, 6), (3, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (7, 6), (7, 6), (5, 6), (7, 6), (6, 6), (5, 6), (5, 6), (4, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (7, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (4, 6), (7, 6), (5, 6), (5, 6), (4, 6), (5, 6), (3, 6), (4, 6), (5, 6), (5, 6), (6, 6), (7, 6), (5, 6), (4, 6), (3, 6), (7, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (4, 6), (7, 6), (3, 6), (6, 6), (5, 6), (6, 6), (4, 6), (4, 6), (6, 6), (3, 6), (6, 6), (5, 6), (6, 6), (4, 6), (6, 6), (3, 6), (7, 6), (3, 6), (6, 6), (3, 6), (6, 6), (7, 6), (5, 6), (7, 6), (6, 6), (3, 6), (3, 6), (5, 6), (7, 6), (6, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (4, 6), (7, 6), (7, 6), (4, 6), (5, 6), (6, 6), (4, 6), (6, 6), (5, 6), (4, 6), (5, 6), (6, 6), (7, 6), (2, 6), (4, 6), (3, 6), (6, 6), (3, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (4, 6), (5, 6), (6, 6), (6, 6), (5, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (3, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (5, 6), (7, 6), (4, 6), (5, 6), (4, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (6, 6), (5, 6), (3, 6), (5, 6), (6, 6), (4, 6), (6, 6), (5, 6), (4, 6), (5, 6), (7, 6), (5, 6), (4, 6), (7, 6), (6, 6), (6, 6), (3, 6), (3, 6), (7, 6), (4, 6), (7, 6), (4, 6), (6, 6), (5, 6), (5, 6), (6, 6), (5, 6), (1, 6), (6, 6), (4, 6), (4, 6), (4, 6), (5, 6), (6, 6), (5, 6), (7, 6), (6, 6), (4, 6), (7, 6), (7, 6), (5, 6), (6, 6), (6, 6), (7, 6), (5, 6), (5, 6), (4, 6), (3, 6), (5, 6), (4, 6), (5, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (6, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (7, 6), (5, 6), (2, 6), (3, 6), (5, 6), (4, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (6, 6), (3, 6), (4, 6), (5, 6), (4, 6), (7, 6), (4, 6), (3, 6), (6, 6), (6, 6), (7, 6), (3, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (7, 6), (5, 6), (5, 6), (6, 6), (3, 6), (4, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (4, 6), (3, 6), (6, 6), (1, 6), (3, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (5, 6), (3, 6), (6, 6), (4, 6), (4, 6), (6, 6), (4, 6), (6, 6), (7, 6), (4, 6), (3, 6), (3, 6), (5, 6), (6, 6), (5, 6), (6, 6), (2, 6), (6, 6), (5, 6), (3, 6), (7, 6), (5, 6), (6, 6), (3, 6), (6, 6), (5, 6), (5, 6), (3, 6), (1, 6), (5, 6), (4, 6), (6, 6), (2, 6), (5, 6), (4, 6), (5, 6), (6, 6), (5, 6), (7, 6), (7, 6), (5, 6), (4, 6), (6, 6), (5, 6), (5, 6), (6, 6), (8, 6), (4, 6), (6, 6), (6, 6), (1, 6), (6, 6), (5, 6), (5, 6), (7, 6), (6, 6), (7, 6), (7, 6), (6, 6), (7, 6), (5, 6), (6, 6), (4, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (4, 6), (5, 6), (6, 6), (4, 6), (4, 6), (4, 6), (5, 6), (7, 6), (5, 6), (7, 6), (5, 6), (7, 6), (6, 6), (7, 6), (5, 6), (4, 6), (6, 6), (6, 6), (3, 6), (5, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (4, 6), (6, 6), (6, 6), (6, 6), (5, 6), (5, 6), (4, 6), (7, 6), (5, 6), (6, 6), (5, 6), (1, 6), (5, 6), (4, 6), (7, 6), (6, 6), (7, 6), (4, 6), (4, 6), (7, 6), (5, 6), (4, 6), (6, 6), (4, 6), (6, 6), (7, 6), (7, 6), (4, 6), (4, 6), (7, 6), (7, 6), (6, 6), (4, 6), (7, 6), (2, 6), (7, 6), (3, 6), (5, 6), (3, 6), (5, 6), (6, 6), (4, 6), (4, 6), (4, 6), (5, 6), (3, 6), (6, 6), (3, 6), (4, 6), (6, 6), (6, 6), (6, 6), (4, 6), (4, 6), (4, 6), (4, 6), (3, 6), (8, 6), (4, 6), (5, 6), (6, 6), (5, 6), (3, 6), (6, 6), (6, 6), (5, 6), (5, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (5, 6), (7, 6), (5, 6), (6, 6), (5, 6), (5, 6), (3, 6), (7, 6), (4, 6), (5, 6), (5, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (4, 6), (7, 6), (7, 6), (6, 6), (3, 6), (5, 6), (4, 6), (3, 6), (5, 6), (6, 6), (6, 6), (4, 6), (5, 6), (5, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (7, 6), (7, 6), (7, 6), (5, 6), (6, 6), (6, 6), (3, 6), (6, 6), (2, 6), (7, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (4, 6), (4, 6), (5, 6), (3, 6), (6, 6), (5, 6), (3, 6), (5, 6), (6, 6), (6, 6), (3, 6), (7, 6), (4, 6), (2, 6), (7, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (7, 6), (4, 6), (5, 6), (4, 6), (5, 6), (4, 6), (7, 6), (5, 6), (5, 6), (6, 6), (6, 6), (6, 6), (4, 6), (5, 6), (6, 6), (7, 6), (4, 6), (4, 6), (6, 6), (4, 6), (4, 6), (8, 6), (4, 6), (7, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (3, 6), (4, 6), (6, 6), (7, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (7, 6), (3, 6), (3, 6), (6, 6), (5, 6), (4, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (6, 6), (8, 6), (5, 6), (6, 6), (3, 6), (4, 6), (2, 6), (7, 6), (5, 6), (7, 6), (5, 6), (8, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (3, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (3, 6), (6, 6), (7, 6), (5, 6), (7, 6), (6, 6), (7, 6), (4, 6), (3, 6), (4, 6), (6, 6), (5, 6), (3, 6), (6, 6), (7, 6), (6, 6), (5, 6), (6, 6), (5, 6), (5, 6), (6, 6), (7, 6), (5, 6), (5, 6), (6, 6), (4, 6), (6, 6), (4, 6), (4, 6), (4, 6), (6, 6), (6, 6), (5, 6), (7, 6), (3, 6), (5, 6), (4, 6), (5, 6), (7, 6), (5, 6), (5, 6), (5, 6), (6, 6), (4, 6), (4, 6), (5, 6), (4, 6), (6, 6), (3, 6), (5, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (7, 6), (6, 6), (4, 6), (7, 6), (6, 6), (7, 6), (6, 6), (4, 6), (3, 6), (6, 6), (3, 6), (4, 6), (6, 6), (6, 6), (3, 6), (6, 6), (6, 6), (7, 6), (6, 6), (3, 6), (5, 6), (5, 6), (4, 6), (6, 6), (4, 6), (6, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (4, 6), (5, 6), (1, 6), (4, 6), (5, 6), (6, 6), (5, 6), (5, 6), (5, 6), (4, 6), (6, 6), (7, 6), (4, 6), (3, 6), (6, 6), (7, 6), (5, 6), (4, 6), (7, 6), (5, 6), (6, 6), (4, 6), (6, 6), (3, 6), (7, 6), (6, 6), (7, 6), (4, 6), (6, 6), (6, 6), (3, 6), (3, 6), (8, 6), (7, 6), (7, 6), (5, 6), (5, 6), (3, 6), (6, 6), (5, 6), (6, 6), (4, 6), (6, 6), (5, 6), (6, 6), (5, 6), (7, 6), (6, 6), (7, 6), (6, 6), (6, 6), (4, 6), (3, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (3, 6), (6, 6), (4, 6), (7, 6), (7, 6), (4, 6), (7, 6), (5, 6), (7, 6), (4, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (4, 6), (6, 6), (5, 6), (4, 6), (6, 6), (6, 6), (7, 6), (7, 6), (6, 6), (3, 6), (6, 6), (5, 6), (4, 6), (7, 6), (5, 6), (5, 6), (7, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (4, 6), (7, 6), (6, 6), (7, 6), (5, 6), (7, 6), (5, 6), (4, 6), (5, 6), (4, 6), (4, 6), (5, 6), (6, 6), (4, 6), (2, 6), (4, 6), (3, 6), (7, 6), (4, 6), (6, 6), (6, 6), (6, 6), (6, 6), (5, 6), (4, 6), (6, 6), (4, 6), (5, 6), (5, 6), (6, 6), (6, 6), (5, 6), (3, 6), (5, 6), (5, 6), (6, 6), (6, 6), (7, 6), (4, 6), (5, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (4, 6), (5, 6), (6, 6), (5, 6), (6, 6), (4, 6), (4, 6), (8, 6), (4, 6), (6, 6), (6, 6), (5, 6), (3, 6), (4, 6), (6, 6), (5, 6), (6, 6), (6, 6), (6, 6), (7, 6), (7, 6), (5, 6), (6, 6), (3, 6), (6, 6), (4, 6), (7, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (4, 6), (5, 6), (5, 6), (6, 6), (5, 6), (4, 6), (8, 6), (5, 6), (6, 6), (6, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (7, 6), (6, 6), (6, 6), (6, 6), (3, 6), (6, 6), (6, 6), (3, 6), (4, 6), (4, 6), (3, 6), (5, 6), (4, 6), (6, 6), (7, 6), (4, 6), (5, 6), (5, 6), (4, 6), (7, 6), (6, 6), (6, 6), (3, 6), (5, 6), (7, 6), (6, 6), (6, 6), (6, 6), (6, 6), (4, 6), (3, 6), (3, 6), (5, 6), (6, 6), (5, 6), (6, 6), (3, 6), (4, 6), (6, 6), (7, 6), (5, 6), (6, 6), (5, 6), (6, 6), (3, 6), (7, 6), (7, 6), (6, 6), (5, 6), (4, 6), (5, 6), (6, 6), (7, 6), (6, 6), (4, 6), (6, 6), (3, 6), (7, 6), (6, 6), (4, 6), (5, 6), (4, 6), (5, 6), (7, 6), (3, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 6), (5, 6), (4, 6), (4, 6), (7, 6), (6, 6), (6, 6), (4, 6), (5, 6), (3, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (2, 6), (4, 6), (5, 6), (4, 6), (4, 6), (7, 6), (4, 6), (5, 6), (5, 6), (7, 6), (5, 6), (5, 6), (4, 6), (4, 6), (5, 6), (4, 6), (6, 6), (6, 6), (5, 6), (6, 6), (3, 6), (3, 6), (4, 6), (6, 6), (5, 6), (7, 6), (7, 6), (4, 6), (6, 6), (7, 6), (5, 6), (5, 6), (5, 6), (7, 6), (4, 6), (6, 6), (3, 6), (5, 6), (7, 6), (6, 6), (3, 6), (3, 6), (6, 6), (4, 6), (4, 6), (3, 6), (6, 6), (3, 6), (6, 6), (6, 6), (6, 6), (7, 6), (6, 6), (6, 6), (4, 6), (6, 6), (6, 6), (7, 6), (5, 6), (7, 6), (3, 6), (3, 6), (7, 6), (6, 6), (4, 6), (7, 6), (6, 6), (6, 6), (3, 6), (4, 6), (4, 6), (5, 6), (6, 6), (7, 6), (6, 6), (3, 6), (3, 6), (6, 6), (5, 6), (6, 6), (7, 6), (6, 6), (5, 6), (6, 6), (5, 6), (6, 6), (6, 6), (5, 6), (5, 6), (5, 6), (5, 6), (5, 6), (7, 6), (4, 6), (5, 6), (7, 6), (3, 6), (4, 6), (5, 6), (4, 6), (7, 6), (5, 6), (5, 6), (6, 6), (7, 6), (6, 6), (4, 6), (7, 6), (6, 6), (3, 6), (7, 6), (6, 6), (3, 6), (5, 6), (5, 6), (5, 6), (5, 6), (3, 6), (6, 6), (6, 6), (7, 6), (5, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (4, 6), (4, 6), (6, 6), (6, 6), (4, 6)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JSjrgO1kzBs3","executionInfo":{"status":"ok","timestamp":1604645934458,"user_tz":420,"elapsed":2312,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"fb16bb65-7942-484d-f1fd-80d6b35f9f87","colab":{"base_uri":"https://localhost:8080/"}},"source":["confusion_matrix = [[0 for _ in range(num_of_labels)] for _ in range(num_of_labels)]\n","for true, actual in zip(y_true,y_pred):\n","  confusion_matrix[true][actual] += 1\n","\n","print(confusion_matrix)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 13, 0, 0], [0, 0, 0, 0, 0, 0, 41, 0, 0], [0, 0, 0, 0, 0, 0, 302, 0, 0], [0, 0, 0, 0, 0, 0, 568, 0, 0], [0, 0, 0, 0, 0, 0, 856, 0, 0], [0, 0, 0, 0, 0, 0, 1133, 0, 0], [0, 0, 0, 0, 0, 0, 461, 0, 0], [0, 0, 0, 0, 0, 0, 22, 0, 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4v1ynuvI0F7Y","executionInfo":{"status":"ok","timestamp":1604645934458,"user_tz":420,"elapsed":2308,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}}},"source":["conf_matrix = pd.DataFrame(confusion_matrix, columns=[c for c in classes], index=[c for c in classes])"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvnzEDIH0fqE","executionInfo":{"status":"ok","timestamp":1604645934873,"user_tz":420,"elapsed":2719,"user":{"displayName":"Soujanya Ranganatha Bhat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYZYrqnugKHbgoo144GZ9rzmvTfTGIL9eFkBCz=s64","userId":"15617339232293464832"}},"outputId":"846b8941-a2e6-4078-8f79-58ab756b1601","colab":{"base_uri":"https://localhost:8080/","height":328}},"source":["conf_matrix"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1920</th>\n","      <th>1930</th>\n","      <th>1940</th>\n","      <th>1950</th>\n","      <th>1960</th>\n","      <th>1970</th>\n","      <th>1980</th>\n","      <th>1990</th>\n","      <th>2000</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1920</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1930</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1940</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>41</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1950</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>302</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1960</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>568</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1970</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>856</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1980</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1133</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1990</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>461</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2000</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      1920  1930  1940  1950  1960  1970  1980  1990  2000\n","1920     0     0     0     0     0     0     1     0     0\n","1930     0     0     0     0     0     0    13     0     0\n","1940     0     0     0     0     0     0    41     0     0\n","1950     0     0     0     0     0     0   302     0     0\n","1960     0     0     0     0     0     0   568     0     0\n","1970     0     0     0     0     0     0   856     0     0\n","1980     0     0     0     0     0     0  1133     0     0\n","1990     0     0     0     0     0     0   461     0     0\n","2000     0     0     0     0     0     0    22     0     0"]},"metadata":{"tags":[]},"execution_count":28}]}]}